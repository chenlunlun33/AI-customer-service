{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import time\n",
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# LLM\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# line\n",
    "from waitress import serve\n",
    "from flask import Flask, request\n",
    "from linebot import LineBotApi, WebhookHandler\n",
    "from linebot.models import TextSendMessage\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELS():\n",
    "    def __init__(self) -> None:\n",
    "        self.basic_auth = ''\n",
    "        self.position = ''\n",
    "        self.query_keywords = None\n",
    "        self.es = None\n",
    "    \n",
    "    # 建立 elastic 連結\n",
    "    def login(self):\n",
    "        self.es = Elasticsearch(\n",
    "            [self.position],\n",
    "            basic_auth=('elastic', self.basic_auth),\n",
    "            verify_certs=False\n",
    "        )\n",
    "\n",
    "    # 載入關鍵字表\n",
    "    def load_key_syn_list(self):\n",
    "        # 讀取關鍵字文件\n",
    "        with open('key_syn\\keywords.txt', 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        self.query_keywords = [line.strip() for line in lines]\n",
    "\n",
    "    # 時間分數遞減\n",
    "    def calculate_time_weight(self, timestamp):\n",
    "        # 抓離現在的時間遠近\n",
    "        current_time = datetime.now()\n",
    "        doc_time = datetime.strptime(timestamp, \"%Y%m%d\")\n",
    "        days_diff = (current_time - doc_time).days\n",
    "        \n",
    "        if days_diff == 0:\n",
    "            # 避免除以零\n",
    "            days_diff = 1\n",
    "        \n",
    "        time_weight = 1 / (math.log(days_diff + 1, 2)+1)  # 避免 log(0) 的錯誤\n",
    "        return time_weight\n",
    "    \n",
    "    # 中文切詞\n",
    "    def tokenize(self, text):\n",
    "        return list(jieba.cut(text))\n",
    "\n",
    "    # 文本搜尋\n",
    "    def search(self, index_name, scaned_keywords, content_keywords, category_keywords, txt, score_threshold=10):\n",
    "        documents = []\n",
    "        # 計算300天前的日期\n",
    "        date_threshold = (datetime.now() - timedelta(days=300)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        if scaned_keywords:\n",
    "            # 針對重要關鍵詞進行查詢\n",
    "            body = {\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\": [\n",
    "                            {\"range\": {\"timestamp\": {\"gte\": date_threshold}}}\n",
    "                        ],\n",
    "                        \"should\": [{\"match\": {\"keywords\": keyword}} for keyword in self.query_keywords],\n",
    "                        \"minimum_should_match\": 1\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 關鍵字搜尋\n",
    "            res = self.es.search(index=index_name, body=body)\n",
    "            for hit in res['hits']['hits']:\n",
    "                doc_content = hit[\"_source\"][\"content\"]\n",
    "                doc_timestamp = hit[\"_source\"].get(\"timestamp\", \"1970-01-01\")\n",
    "                doc_keywords = hit[\"_source\"].get(\"keywords\", [])\n",
    "                documents.append({\n",
    "                    'content': doc_content,\n",
    "                    'timestamp': doc_timestamp,\n",
    "                    'keywords': doc_keywords,\n",
    "                    'id': hit[\"_id\"]\n",
    "                })\n",
    "\n",
    "        if not documents:\n",
    "            return ' ', None, None\n",
    "        \n",
    "        # 從txt中提取出與self.query_keywords匹配的關鍵字\n",
    "        tokenized_query = [keyword for keyword in self.query_keywords if keyword in txt]\n",
    "        \n",
    "        # 如果沒有匹配的關鍵字，返回'無相關內容'\n",
    "        if not tokenized_query:\n",
    "            return ' ', None, None\n",
    "\n",
    "        # 對文本進行分詞\n",
    "        tokenized_corpus = [self.tokenize(doc['content']) for doc in documents]\n",
    "        \n",
    "        # 建立 bm25\n",
    "        bm25 = BM25Okapi(tokenized_corpus, k1=1, b=0.75)\n",
    "        \n",
    "        # 計算 bm25 分數\n",
    "        all_scores = bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # 把分數和文本放一起\n",
    "        scores = []\n",
    "        for idx, doc in enumerate(documents):\n",
    "            score = all_scores[idx]\n",
    "            time_weight = self.calculate_time_weight(doc['timestamp'])\n",
    "            final_score = score / time_weight\n",
    "            scores.append((final_score, doc['content'], doc['timestamp'], doc['keywords'], doc['id']))\n",
    "\n",
    "        # 初排（文本分數/時間遞減）\n",
    "        scores.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        # 類別關鍵字優先排序\n",
    "        category_priority_ranking = []\n",
    "        non_category_ranking = []\n",
    "        for score, content, timestamp, doc_keywords, doc_id in scores:\n",
    "            if any(keyword in category_keywords for keyword in doc_keywords):\n",
    "                # 匹配到的放 category_priority_ranking\n",
    "                category_priority_ranking.append((score, content, timestamp, doc_keywords, doc_id))\n",
    "            else:\n",
    "                # 未匹配到的放 non_category_ranking\n",
    "                non_category_ranking.append((score, content, timestamp, doc_keywords, doc_id))\n",
    "\n",
    "        # 合併保持排序順序\n",
    "        final_ranking = category_priority_ranking + non_category_ranking\n",
    "\n",
    "        # 內容關鍵字次優先排序\n",
    "        content_priority_ranking = []\n",
    "        non_content_ranking = []\n",
    "        for score, content, timestamp, doc_keywords, doc_id in final_ranking:\n",
    "            if any(keyword in content_keywords for keyword in doc_keywords):\n",
    "                # 匹配到的 content_priority_ranking\n",
    "                content_priority_ranking.append((score, content, timestamp, doc_keywords, doc_id))\n",
    "            else:\n",
    "                # 未匹配到的 non_content_ranking\n",
    "                non_content_ranking.append((score, content, timestamp, doc_keywords, doc_id))\n",
    "\n",
    "        # 合併保持排序順序\n",
    "        final_ranking = content_priority_ranking + non_content_ranking\n",
    "\n",
    "        # 檢查分數是否低於閾值\n",
    "        max_final_score = final_ranking[0][0] if final_ranking else 0\n",
    "        best_document = final_ranking[0][1] if final_ranking else '無相關內容'\n",
    "        time = final_ranking[0][2] if final_ranking else None\n",
    "\n",
    "        # 加上時間\n",
    "        best_document = f'資料時間:{time}\\n' + best_document\n",
    "\n",
    "        if max_final_score < score_threshold:\n",
    "            return ' ', max_final_score, time\n",
    "        \n",
    "        return best_document, max_final_score, time\n",
    "    \n",
    "    def search_basic(self, index_name, matched_keywords):\n",
    "        documents = []\n",
    "        \n",
    "        if matched_keywords:\n",
    "            body = {\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": [{\"match\": {\"keywords\": keyword}} for keyword in matched_keywords],\n",
    "                        \"minimum_should_match\": 1\n",
    "                    }\n",
    "                },\n",
    "                \"size\": 10\n",
    "            }\n",
    "            \n",
    "            # 冠鍵字搜尋\n",
    "            res = self.es.search(index=index_name, body=body)\n",
    "            \n",
    "            # 搜尋結果處理\n",
    "            for hit in res['hits']['hits']:\n",
    "                doc_content = hit[\"_source\"][\"content\"]\n",
    "                doc_timestamp = hit[\"_source\"].get(\"timestamp\", \"2024-08-26\")\n",
    "                doc_score = hit[\"_score\"]\n",
    "                documents.append((doc_content, doc_timestamp, hit[\"_id\"], doc_score))\n",
    "        \n",
    "        # 找不到文件 返回空字串\n",
    "        if not documents:\n",
    "            return ' '\n",
    "        \n",
    "        # 按 Elasticsearch 内建分数排序\n",
    "        documents.sort(key=lambda x: x[3], reverse=True)\n",
    "        \n",
    "        # 找分數最高者\n",
    "        best_document, best_timestamp, best_id, best_score = documents[0]\n",
    "        \n",
    "        return best_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 客戶資料庫\n",
    "class ClientDeport():\n",
    "    def __init__(self) -> None:\n",
    "        self.client_df = pd.read_csv('client_df.csv')\n",
    "        self.ID_list = [os.path.splitext(filename)[0] for filename in os.listdir('client') if filename.endswith('.json')]\n",
    "\n",
    "    # 取得客戶資料\n",
    "    def get_client_df(self):\n",
    "        self.client_df = pd.read_csv('client_df.csv')\n",
    "    \n",
    "    # 更新客戶資料總表\n",
    "    def renew_client_df(self, name:str, number:str, user_ID:str):\n",
    "        # 新增新資料\n",
    "        new_data = {\n",
    "            '姓名': name,\n",
    "            '電話': number,\n",
    "            '最後更新日期': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user_ID': user_ID,\n",
    "            '認證': '電腦認證'\n",
    "        }\n",
    "        # 新資料轉 df\n",
    "        new_df = pd.DataFrame([new_data])\n",
    "        # 新增到原本 df\n",
    "        self.client_df = pd.concat([self.client_df, new_df], ignore_index=True)\n",
    "        # 儲存 df\n",
    "        self.client_df.to_csv('client_df.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    # ID 比對\n",
    "    def ID_check(self, user_ID):\n",
    "        if user_ID in self.client_df['user_ID'].values:\n",
    "            auth_value = self.client_df.loc[self.client_df['user_ID'] == user_ID, '認證'].values[0]\n",
    "            if auth_value == '電腦認證' or auth_value == '人工認證':\n",
    "                return '比對成功'\n",
    "            return '認證異常'\n",
    "        else:\n",
    "            return '尚無資料'\n",
    "    \n",
    "    # 姓名_電話 擷取\n",
    "    def extract_name_phone(self, message:str):\n",
    "        # 正則表達式\n",
    "        pattern = r'(\\S+?)\\s*[\\r\\n\\s]*([\\d]{10})'\n",
    "        \n",
    "        # 看是否有符合項\n",
    "        match = re.search(pattern, message)\n",
    "        \n",
    "        if match:\n",
    "            # 擷取姓名 電話\n",
    "            name = match.group(1)\n",
    "            phone = match.group(2)\n",
    "            return name, phone\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    # 取得 ID_list\n",
    "    def get_iso_data_list(self):\n",
    "        self.ID_list = [os.path.splitext(filename)[0] for filename in os.listdir('client') if filename.endswith('.json')]\n",
    "\n",
    "    # 創建客戶個人資料庫\n",
    "    def bulid_iso_data(self, user_ID): # 每次有新建立，self裡面的iso_list就要更新，重刷一次\n",
    "        # 初始化json\n",
    "        data = {\n",
    "            \"last_update\": \"\",\n",
    "            \"updates\": []\n",
    "        }\n",
    "        \n",
    "        # 創建新 json\n",
    "        with open(f'client\\{user_ID}.json', 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "\n",
    "    # 更新客戶個人資料庫\n",
    "    def renew_iso_data(self, user_ID, input_text, client_keywords):\n",
    "        # 載入 json 文件\n",
    "        if os.path.exists(f'client\\{user_ID}.json'):\n",
    "            with open(f'client\\{user_ID}.json', 'r') as json_file:\n",
    "                data = json.load(json_file)\n",
    "        else:\n",
    "            print(f'No {user_ID} json deport')\n",
    "        \n",
    "        # 得到當前時間\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # 更新 last_update 時間\n",
    "        data[\"last_update\"] = current_time\n",
    "        \n",
    "        # 新的紀錄\n",
    "        new_record = {\n",
    "            \"text\": input_text,\n",
    "            \"time\": current_time,\n",
    "            \"keywords\": client_keywords\n",
    "        }\n",
    "        # 把新紀錄新增進去\n",
    "        data[\"updates\"].append(new_record)\n",
    "        \n",
    "        # 儲存更新後的 json\n",
    "        with open(f'client\\{user_ID}.json', 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, StoppingCriteria\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence=[128256]):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_ids = input_ids[:, -len(self.eos_sequence):].tolist()\n",
    "        return self.eos_sequence in last_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語言模型 class(模型載入、回答輸出、數據資料庫串接)\n",
    "class Llama3():\n",
    "    def __init__(self) -> None:\n",
    "        self.pipe = None\n",
    "        self.terminators = None\n",
    "    \n",
    "    # 載入模型\n",
    "    def load_model(self):\n",
    "        # 本地端部屬 llama3 並加入量化\n",
    "        self.pipe = pipeline(\"text-generation\", model=\"local\",\n",
    "            model_kwargs={\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"quantization_config\": {\"load_in_4bit\": True},\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "            },\n",
    "        )\n",
    "        self.terminators = [\n",
    "            self.pipe.tokenizer.eos_token_id,\n",
    "            self.pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "    \n",
    "    # message json 的打包\n",
    "    def message(self, info, company_text, stock_data, Q):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{Q}\"},\n",
    "        ]\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    # 回答\n",
    "    def answer(self, messages:json):\n",
    "        outputs = self.pipe(\n",
    "            messages,\n",
    "            max_new_tokens=1024,  # 減少生成文本的最大長度\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.6,  # 減少top_p值以提高速度\n",
    "        )\n",
    "        response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # 抓數據庫資料\n",
    "    def call_data_deport(self, keywords_pairs):\n",
    "        def search_stock(stock_id):\n",
    "            # 查找股票代號對應的行\n",
    "            df = pd.read_csv(r'data_deport\\newest.csv')\n",
    "            stock_data = df[df[\"股票代號\"] == stock_id]\n",
    "    \n",
    "            if stock_data.empty:\n",
    "                return \"未找到該股票代號的資料\"\n",
    "    \n",
    "            # 提取需要的欄位\n",
    "            fields = [\n",
    "                \"單月合併營收(千)\",\n",
    "                \"單月合併營收年成長(%)\", \n",
    "                \"單月合併營收月變動(%)\", \n",
    "                \"累計合併營收(千)\", \n",
    "                \"近三月合併營收(千)\",\n",
    "                \"近三月合併營收年成長(%)\",\n",
    "                \"近12月營收合併成長(%)\"\n",
    "            ]\n",
    "\n",
    "            date = stock_data[\"年月\"].values[0]\n",
    "            # 取出數據，轉換成字串格式\n",
    "            result = ', '.join([f\"{field}: {stock_data[field].values[0]}\" for field in fields])\n",
    "    \n",
    "            return result, date\n",
    "\n",
    "        # 如果沒有股票代碼就直接跑客服\n",
    "        if len(keywords_pairs) == 0:\n",
    "            return ''\n",
    "\n",
    "        # 如果有股票代碼就抓資料庫\n",
    "        else:\n",
    "            stock_data = '最新營收資料:\\n'\n",
    "            for i in keywords_pairs:\n",
    "                # 抓數據找自己要的股號\n",
    "                try:\n",
    "                    result_string, date = search_stock(i[0])\n",
    "                    result_string = f'{i[1]} 最新{date}營收資料:' + result_string\n",
    "                except:\n",
    "                    result_string = ''\n",
    "                # 加入stock_data裡面\n",
    "                stock_data += result_string\n",
    "        \n",
    "        return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料預處理 class(第一層預處理(大小寫、停用字)、關鍵字掃描、同義詞轉換)\n",
    "class DataProcess():\n",
    "    def __init__(self) -> None:\n",
    "        self.stocks_df = pd.read_csv('key_syn\\stocks.csv')\n",
    "        self.keywords_content_list = None\n",
    "        self.keywords_type_list = None\n",
    "        self.keywords_basic_list = None\n",
    "        self.keywords_list = None\n",
    "        self.syn_json = None\n",
    "\n",
    "    # 大小寫轉換、停用字\n",
    "    def basic(self, input_text:str):\n",
    "        # # 去除特殊字元及標點\n",
    "        # text = re.sub(r'[^\\w\\s]', '', input_text)\n",
    "\n",
    "        # 大小寫轉換(全轉成小寫)\n",
    "        text = re.sub(r'[A-Za-z]+', lambda x: x.group().lower(), input_text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    # 抓文本股票代號、名子\n",
    "    def scan_stock(self, input_text:str):\n",
    "        # 將 'stock_id' int 轉 str\n",
    "        self.stocks_df['stock_id'] = self.stocks_df['stock_id'].astype(str)\n",
    "        # 將 self.stocks_df 轉為字典 用 'stock_id' 作 key 'name' 作 value\n",
    "        stock_info = dict(zip(self.stocks_df['stock_id'], self.stocks_df['name']))\n",
    "\n",
    "        # 掃描股號、股名\n",
    "        keywords_pairs = []\n",
    "        for code, name in stock_info.items():\n",
    "            if code in input_text:\n",
    "                keywords_pairs.append((code, name))\n",
    "            if name in input_text:\n",
    "                keywords_pairs.append((code, name))\n",
    "\n",
    "        # 使用 set 去掉重複項再轉回 list\n",
    "        keywords_pairs = list(set(map(tuple, keywords_pairs)))\n",
    "        # 將每對 (code, name) 轉換為列表形式\n",
    "        keywords_pairs = [list(pair) for pair in keywords_pairs]\n",
    "\n",
    "        return keywords_pairs\n",
    "    \n",
    "    # 載入關鍵字表\n",
    "    def load_key_syn_list(self):\n",
    "        # 讀取關鍵字文件\n",
    "        with open('key_syn\\keywords.txt', 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            self.keywords_list = [line.strip() for line in lines]\n",
    "\n",
    "        # 讀取關鍵字文件\n",
    "        with open('key_syn\\keywords_content.txt', 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            self.keywords_content_list = [line.strip() for line in lines]\n",
    "\n",
    "        # 讀取關鍵字文件\n",
    "        with open('key_syn\\keywords_type.txt', 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            self.keywords_type_list = [line.strip() for line in lines]\n",
    "\n",
    "        # 讀取關鍵字文件\n",
    "        with open('key_syn\\keywords_basic.txt', 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            self.keywords_basic_list = [line.strip() for line in lines]\n",
    "        \n",
    "        # 讀取同義詞 json\n",
    "        with open('key_syn\\synonym_mapping.json', 'r', encoding='utf-8') as json_file:\n",
    "            self.syn_json = json.load(json_file)\n",
    "\n",
    "    # 掃描關鍵字\n",
    "    def scan_type_keyword(self, input_text:str):\n",
    "        extracted_keywords = []\n",
    "        for keyword in self.keywords_type_list:\n",
    "            if keyword in input_text:\n",
    "                extracted_keywords.append(keyword)\n",
    "\n",
    "        return extracted_keywords\n",
    "    \n",
    "    # 掃描關鍵字\n",
    "    def scan_content_keyword(self, input_text:str):\n",
    "        extracted_keywords = []\n",
    "        for keyword in self.keywords_content_list:\n",
    "            if keyword in input_text:\n",
    "                extracted_keywords.append(keyword)\n",
    "\n",
    "        return extracted_keywords\n",
    "    \n",
    "    def scan_basic_keyword(self, input_text:str):\n",
    "        extracted_keywords = []\n",
    "        for keyword in self.keywords_basic_list:\n",
    "            if keyword in input_text:\n",
    "                extracted_keywords.append(keyword)\n",
    "\n",
    "        return extracted_keywords\n",
    "    \n",
    "    # 同義詞轉換\n",
    "    def convert_synonym(self, extracted_keywords: list):\n",
    "        unique_keywords = set()\n",
    "        for keyword in extracted_keywords:\n",
    "            transformed_keyword = self.syn_json.get(keyword, keyword)\n",
    "            unique_keywords.add(transformed_keyword)\n",
    "        return list(unique_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_text = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line運作@app (抓ID)\n",
    "# 帳務\n",
    "access_token = ''\n",
    "secret = ''\n",
    "# 確認 token secret\n",
    "line_bot_api = LineBotApi(access_token)\n",
    "handler = WebhookHandler(secret)\n",
    "\n",
    "# 眾多的 class\n",
    "CD = ClientDeport()\n",
    "DP = DataProcess()\n",
    "DP.load_key_syn_list()\n",
    "ES = ELS()\n",
    "ES.login()\n",
    "ES.load_key_syn_list()\n",
    "LL = Llama3()\n",
    "LL.load_model()\n",
    "\n",
    "def schedule_task(interval):\n",
    "    \"\"\"\n",
    "    每隔指定時間 interval(秒) 來執行 func 函式\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        time.sleep(interval)\n",
    "        CD.get_client_df()\n",
    "        CD.get_iso_data_list()\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\", methods=['POST'])\n",
    "def linebot():\n",
    "    # 取得訊息內容\n",
    "    body = request.get_data(as_text=True)\n",
    "    try:\n",
    "        # 訊息轉 json\n",
    "        json_data = json.loads(body)\n",
    "        # header 處理\n",
    "        signature = request.headers['X-Line-Signature']\n",
    "        handler.handle(body, signature)\n",
    "        \n",
    "        # 取得回傳訊息的 Token\n",
    "        tk = json_data['events'][0]['replyToken']\n",
    "        # 取得 LINE 收到的訊息類型\n",
    "        type = json_data['events'][0]['message']['type']\n",
    "        # 取得 user_id\n",
    "        user_id = json_data['events'][0]['source']['userId']\n",
    "\n",
    "        if type=='text':\n",
    "            # 取得 LINE 收到的文字訊息\n",
    "            msg = json_data['events'][0]['message']['text']\n",
    "\n",
    "            # 刪除空格及換行\n",
    "            msg_cleaned = re.sub(r'\\s+', '', msg)\n",
    "            # 檢查開頭是否有 \"\"\n",
    "            prefix = \"\"\n",
    "            if msg_cleaned.startswith(prefix):\n",
    "                msg_cleaned = msg_cleaned[3:]\n",
    "                # ---------------------------------\n",
    "                # 1.比對ID\n",
    "                # 比對文字\n",
    "                name, phone = CD.extract_name_phone(msg_cleaned)\n",
    "                \n",
    "                # 認證異常請她連絡理專\n",
    "                if CD.ID_check(user_id) == '認證異常':\n",
    "                    line_bot_api.reply_message(tk, TextSendMessage('認證異常'))\n",
    "                    \n",
    "                # 無資料比對msg比對成功建檔並接續語言模型 失敗就請他回傳正確格式\n",
    "                elif CD.ID_check(user_id) == '尚無資料' and name == None:\n",
    "                    line_bot_api.reply_message(tk, TextSendMessage('請先依格式給予 姓名_手機號碼'))\n",
    "                    \n",
    "                # 其他的為認證成功或初次建檔成功\n",
    "                else:\n",
    "                    # 初建資料庫的\n",
    "                    if user_id not in CD.ID_list:\n",
    "                        print(f'新建資料{user_id}')\n",
    "                        CD.renew_client_df(name, phone, user_id)\n",
    "                        CD.bulid_iso_data(user_id)\n",
    "                        CD.get_client_df()\n",
    "                        CD.get_iso_data_list()\n",
    "                        line_bot_api.reply_message(tk, TextSendMessage(f'初建檔案姓名:{name} 電話:{phone}'))\n",
    "                    \n",
    "                    else:\n",
    "                        # 2.文字處理\n",
    "                        input_text = DP.basic(msg_cleaned)\n",
    "                        scan_stock_list = DP.scan_stock(msg_cleaned)\n",
    "                        scan_type_keywords_list = DP.scan_type_keyword(msg_cleaned)\n",
    "                        scan_type_keywords_list = DP.convert_synonym(scan_type_keywords_list)\n",
    "                        scan_content_keywords_list = DP.scan_content_keyword(msg_cleaned)\n",
    "                        scan_content_keywords_list = DP.convert_synonym(scan_content_keywords_list)\n",
    "                        scan_keywords_list = scan_content_keywords_list + scan_type_keywords_list\n",
    "                        scan_basic_keywords_list = DP.scan_basic_keyword(msg_cleaned)\n",
    "                        scan_basic_keywords_list = DP.convert_synonym(scan_basic_keywords_list)\n",
    "                        \n",
    "                        # 3.更新客戶資料庫\n",
    "                        CD.renew_iso_data(user_id, input_text, scan_keywords_list)\n",
    "\n",
    "                        # 4.文本、數據資料庫\n",
    "                        best_doc, score, time = ES.search('txtdeport', scan_keywords_list, scan_content_keywords_list, scan_type_keywords_list, msg_cleaned)\n",
    "                        print(best_doc)\n",
    "                        if scan_basic_keywords_list != []:\n",
    "                            info = basic_text\n",
    "                        else:\n",
    "                            info = ''\n",
    "\n",
    "                        stock_data_txt = LL.call_data_deport(scan_stock_list)\n",
    "                        print(stock_data_txt)\n",
    "                        # 5.語言模型\n",
    "                        message = LL.message(info, best_doc, stock_data_txt, input_text)\n",
    "                        answer = LL.answer(message)\n",
    "\n",
    "                        # 回傳訊息\n",
    "                        print(answer)\n",
    "                        line_bot_api.reply_message(tk, TextSendMessage(answer))\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    # 發生錯誤 print 問題\n",
    "    except:\n",
    "        print('error', body)\n",
    "\n",
    "    # 驗證 Webhook 使用，不能省略\n",
    "    return 'OK'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 開啟一個新的執行緒來每1分鐘執行一次 load_key_syn_list\n",
    "    thread = threading.Thread(target=schedule_task, args=(300,))\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "    \n",
    "    # 啟動 Flask 應用\n",
    "    # app.run()\n",
    "    serve(app, host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
